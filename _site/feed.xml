<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-02-08T20:20:28+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">척척 석사</title><subtitle></subtitle><author><name>charcoalSkies</name></author><entry><title type="html">github.io 블로그 시작하기</title><link href="http://localhost:4000/blog/first-post/" rel="alternate" type="text/html" title="github.io 블로그 시작하기" /><published>2023-02-08T00:00:00+09:00</published><updated>2023-02-08T22:06:00+09:00</updated><id>http://localhost:4000/blog/first-post</id><content type="html" xml:base="http://localhost:4000/blog/first-post/"><![CDATA[<p>GitHub Blog 서비스인 github.io 블로그 시작하기로 했다.
GitHub Blog 서비스의 이름은 Pages이다.</p>

<p>Pages가 다른 블로그 플랫폼 보다 편한 것 같아서 마음에 든다.
다른 사람들도 같이 많이 사용했으면 좋겠다는 생각이 든다.</p>

<p>YFM에서 정의한 제목을 이중 괄호 구문으로 본문에 추가할 수 있다.
이 글의 제목은 github.io 블로그 시작하기이고
마지막으로 수정된 시간은 2023-02-08 08:06:00 -0500이다.</p>]]></content><author><name>charcoalSkies</name></author><category term="Blog" /><category term="Blog" /><summary type="html"><![CDATA[GitHub Blog 서비스인 github.io 블로그 post 테스트]]></summary></entry><entry><title type="html">DCGAN 논문 리뷰</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/dcgan/" rel="alternate" type="text/html" title="DCGAN 논문 리뷰" /><published>2023-02-08T00:00:00+09:00</published><updated>2023-02-08T22:06:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/dcgan</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/dcgan/"><![CDATA[]]></content><author><name>charcoalSkies</name></author><category term="논문 리뷰" /><category term="DCGAN" /><category term="리뷰" /><category term="논문" /><category term="review" /><summary type="html"><![CDATA[Deeo Convolutional Generative Adversarial Networks]]></summary></entry><entry><title type="html">GAN 논문 리뷰</title><link href="http://localhost:4000/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/gan/" rel="alternate" type="text/html" title="GAN 논문 리뷰" /><published>2023-02-08T00:00:00+09:00</published><updated>2023-02-08T22:06:00+09:00</updated><id>http://localhost:4000/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/gan</id><content type="html" xml:base="http://localhost:4000/%EB%85%BC%EB%AC%B8%20%EB%A6%AC%EB%B7%B0/gan/"><![CDATA[<table>
  <thead>
    <tr>
      <th> </th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>태그</td>
      <td>Generative Model</td>
    </tr>
    <tr>
      <td>Description</td>
      <td>GAN</td>
    </tr>
    <tr>
      <td>Journal/Conference</td>
      <td>NIPS</td>
    </tr>
    <tr>
      <td>Link</td>
      <td>https://arxiv.org/abs/1406.2661</td>
    </tr>
    <tr>
      <td>Year</td>
      <td>2014</td>
    </tr>
    <tr>
      <td>원문 링크</td>
      <td>https://arxiv.org/pdf/1406.2661.pdf</td>
    </tr>
  </tbody>
</table>

<h2 id="논문-읽기전-알아두면-좋은-상식">논문 읽기전 알아두면 좋은 상식</h2>

<ul>
  <li>
    <p>GAN Model
<img src="https://drive.google.com/uc?id=1RuRbkqEkzsf87wRNT1LghGKDd-UsDQBK" alt="GAN Image" /></p>
  </li>
  <li>GAN이란?
``` txt</li>
  <li>
    <p>GAN (Generative Adversarial Network)은 딥러닝 모델 중 이미지 생성에 널리 쓰이는 모델</p>
  </li>
  <li>
    <p>CNN은 이미지에서 개인지 고양이인지 구분하는 이미지 분류 문제에 쓰인다면 GAN은 모델이 데이터셋과 유사한 이미지를 만들도록 하는 것
```</p>
  </li>
  <li>이미지 생성 원리
``` txt
경찰과 위조지폐범</li>
</ul>

<p>위조지폐범은 위조지폐를 진짜 지폐와 거의 비슷하게 만들고 경찰은 이 지폐가 진짜 지폐인지 위조지폐인지 구분</p>

<p>위조지폐범과 경찰은 적대적인 관계에 있고, 위조지폐범은 계속 위조지폐를 생성하고 경찰은 진짜를 찾아내려고 하는 쫓고 쫓기는 과정이 반복</p>

<p>이때 위조지폐범(Generator)와 경찰(Discriminator)가 서로 위조지폐를 생성하고 구분하는 것을 반복하는 minmax game</p>

<p>위와 마찬가지로 GAN또한 Generator(생성자)와 Discriminator(판별자) 두 개의 모델이 동시에 적대적인 과정으로 학습</p>

<p>생성자 G는 실제 데이터 분포를 학습하고, 판별자 D는 원래의 데이터인지 생성자로부터 생성된 것인지를 구분</p>

<p>생성자 G의 학습 과정은 이미지를 잘 생성해서 속일 확률을 높이고 판별자 D는 제대로 구분하는 확률을 높이는 두 플레이어의 minmax game의 과정
```</p>

<ul>
  <li>기초 수학
    <ul>
      <li>확률분포
        <ul>
          <li>확률분포는 확률 변수가 특정한 값을 가질 확률을 나타내는 함수를 의미</li>
          <li>예를 들어 주사위를 던졌을 때 나올 수 있는 수를 확률변수 X라고 하면
            <ul>
              <li>확률변수 X는 1, 2, 3, 4, 5, 6의 값을 가질 수 있다.</li>
              <li>P(X = 1)는 $\frac{1}{6}$</li>
              <li>P(X = 1) = P(X=2) = P(X=3) = P(X=4) = P(X=5) = P(X=6)</li>
            </ul>
          </li>
        </ul>
      </li>
      <li>이산확률분포
        <ul>
          <li>확률변수 X의 개수를 정확히 셀 수 있을 때 이산확률분포라 말함</li>
          <li>주사위 눈금 X의 확률 분포는 1 / 6이다</li>
        </ul>
      </li>
      <li>연속확률분포
        <ul>
          <li>확률변수 X의 개수를 정확히 셀 수 없을 때 연속확률분포라 말한다</li>
          <li>ex) 키, 달리기 성적</li>
        </ul>
      </li>
      <li>정규분포
        <ul>
          <li>수집된 자료의 분포를 근사하는데 주로 사용</li>
          <li>가우시안 분포는 연속 확률 분포의 하나.</li>
          <li>실제 세계 많은 데이터는 정규분포로 표현 가능</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/7/74/Normal_Distribution_PDF.svg/1920px-Normal_Distribution_PDF.svg.png" alt="Gaussian distribution" /></p>

<ul>
  <li>이미지 데이터에 대한 확률분포
    <ul>
      <li>이미지 데이터는 다차원 특징 공간의 한 점으로 표현된다.
        <ul>
          <li>이미지의 분포를 근사하는 모델을 학습할 수 있다.</li>
        </ul>
      </li>
      <li>사람의 얼굴에는 통계적인 평균치가 존재할 수 있다.
        <ul>
          <li>모델은 이를 수치적으로 표현할 수 있게 된다.</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>생성 모델(Generative Models)
    <ul>
      <li>생성 모델은 실존하지 않지만 있을 법한 이미지를 생성할 수 있는 모델을 의미</li>
    </ul>
  </li>
  <li>생성 모델의 목표
    <ul>
      <li>이미지 데이터의 분포를 근사하는 모델 G를 만드는 것이 생성모델의 목표</li>
      <li>모델 G가 잘 동작한다는 의미는 원래 이미지들의 분포를 잘 모델링할 수 있다는 것을 의미</li>
    </ul>
  </li>
</ul>

<p><img src="https://drive.google.com/uc?id=1ZDIV1rDmTvpQhiBE6-5eM6JLc3hEeUOS" alt="GAN Image" /></p>

<ul>
  <li>log 특성
    <ul>
      <li>log(1) = 0</li>
      <li>log(0) = -infinity</li>
    </ul>
  </li>
  <li>게임이론 이란
    <ul>
      <li>폰 노이만과 오스카 모겐스턴에 의해 확립된 게임이론</li>
      <li>
        <p>게임이론은 사회전반에서 발생하는 개체와 개체간의 갈등, 경쟁, 대립, 협력등을 수학적 기법을 사용하여 표현하려는 수학의 한 분야</p>
      </li>
      <li>제로섬게임
        <ul>
          <li>승자의 이익과 패자의 손실을 합하면 항상 0</li>
          <li>ex) 가위바위보 : 누군가 이득이 있으면 그 반대는 항상 손실이 있다</li>
        </ul>
      </li>
      <li>minimax game
        <ul>
          <li>누군가 이기면 누군가는 질 수밖에 없는 제로섬 게임에서는 지지 않는 것이 최선의 전략이지만 미니맥스 전략은 상대의 최대값을 최소화 하는것 혹은 졌을 때의 손실을 최소화 하는 것</li>
          <li>맥스의 손실을 미니화 한다는 것</li>
          <li>얘를들면 제로섬 게임의 대표적인 예인 고스톱에서 돈을 따기 위해서는 박부터 피하는 것이 기본인것 처럼 나보다 상대가 어떻게 하면 돈을 따지 못할까에 집중한 전략</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="참고-링크">참고 링크</h3>
<p>https://ysbsb.github.io/gan/2020/06/17/GAN-newbie-guide.html</p>

<p>https://www.youtube.com/watch?v=AVvlDmhHgC4&amp;t=379s</p>

<h2 id="주요-설명">주요 설명</h2>

<h3 id="adversarial-nets">Adversarial Nets</h3>

<p>D and G play the following two-player minimax game with value functiuon $V(G,D):$</p>

\[\min_{G} \; \max_{D} V(D, G) = E_{x \sim p_{data}} [\log D(x)] + E_{z \sim p_z(z)} [\log (1 - D(G(z)))]\]

<ul>
  <li>
    <p>$D$는 $V$함수를 높이고자 vs $G$는 $V$함수를 낮추고자 경쟁 (Real : 1 ~ Fake : 0)</p>
  </li>
  <li>프로그램 상에서 기댓값(expected value)을 계산하는 방법
    <ul>
      <li>= 단순히 모든 데이터를 하나씩 확인해 식에 대입한 뒤 평균값 계산</li>
    </ul>
  </li>
  <li>
    <p>$E_{x \sim p_{data}} [\log D(x)]$ : 원본 데이터 분포 (data distrbution)에서의 sample $x$ 를 뽑아 $\log D(x)$ 의 기댓값 계산</p>
  </li>
  <li>
    <p>$E_{z \sim p_z(z)} [\log (1 - D(G(z)))]$ : 노이즈 분포에서의 sample $z$ 를 뽑아 $\log (1-D(G(z)))$의 기댓값 계산</p>
  </li>
  <li>정리
    <ul>
      <li>D의 입장
        <ul>
          <li>가짜 데이터에는 0을 출력, 진짜 데이터에는 1을 출력</li>
          <li>x는 진짜 데이터, G(z) 는 G가 z를 가지고 만든 가짜 데이터</li>
          <li>
            <p>따라서 D는 오른쪽 수식 중, D(x) = 1이 되어야 하고, D(G(z))는 0이 되도록 하는것이 최대 목표</p>
          </li>
          <li>D가 가장 원하는 상황
            <ul>
              <li>
                <p>$\min_{G} \; \max_{D} V(D, G) = E_{x \sim p_{data}} [\log D(x)] + E_{z \sim p_z(z)} [\log (1 - D(G(z)))]$
  $logD(x)$  에서 $D(x)$ = 1
  $log(1-D(G(z)))$ 에서 $D(G(z))$ = 0
  결국 0을 만드는게 가장 큰 값이며 D(G(z)) 가 0이 되는 상황이 D가 원하는 값 (Real : 1, Fake : 0)</p>
              </li>
              <li>
                <p>따라서 D는 0에 가까운 숫자로 가기위해 노력하는 것을 $max$가 되려고 한다하고, 이를 $max\,D$로  표현한 것</p>
              </li>
              <li>
                <p>G가 가장 원하는 상황</p>
                <ul>
                  <li>$\min_{G} \; \max_{D} V(D, G) = E_{x \sim p_{data}} [\log D(x)] + E_{z \sim p_z(z)} [\log (1 - D(G(z)))]$</li>
                  <li>G는 $E_{x \sim p_{data}} [\log D(x)]$ 는 관심없음</li>
                  <li>뒤에 있는 $D(G(z))$ 가 1 이 되도록 하는것이 목표 (Real : 1, Fake : 0)</li>
                  <li>$log(1-1) = -infinity$</li>
                  <li>결국 G의 목표는 음의 무한대를 향하는 것이 목적</li>
                  <li>즉 G가 원하는 최적의 상황은 매우 작은 음의 무한대가 되는 방향</li>
                </ul>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="global-optimality-of-p_g--p_data">Global Optimality of $p_g = P_{data}$</h3>

<p>임의의 G가 주어졌을 때 최적의 D를 계산하는 과정
\(D_g^*(x) = \frac{P_{data}(x)}{P_{data}(x) + P_g(x)}\)</p>

\[V(G, D) = E_{x \sim p_{data}} [\log D(x)] + E_{z \sim p_z(z)} [\log (1 - D(G(z)))]\]

<ul>
  <li>연속확률변수에 대한 기댓값 $E[X] = \int_{-\infty}^\infty xf(x)dx\; (x: 사건,\; f(x): 확률분포 함수)$
\(V(G,D) = \int_x \mathrm{p_{data}(x) \log(D(x))}\mathrm{d}x + \int_z p_z(z)log(1-D(G(z)))dz\)</li>
</ul>

<p><img src="https://drive.google.com/uc?id=1dh23UDJ3y1SYhQA2i5fiUox69yZtc44x" alt="Image" /></p>

<p><img src="https://drive.google.com/uc?id=1XL_qLM4Hvdt1ZQzmxLkDxeIt-AhrQVSd" alt="Image" /></p>

<p>z 도메인에서 Sampling된 noise vector를 G에 넣어서 데이터 x를 만들어낼 수 있고 이는 도메인 z에서 x로 맵핑되는 과정과 마친가지로 볼 수 있기 때문에 x로 치환해서 하나의 적분 식으로 표현 가능<br />
\(= \int_x P_{data}(x) log(D(x)) + P_g(x)\log(1-D(x))dx\)</p>

\[function\; y = alog(y) + blog(1-y)\; achieves\; its\; maximum\; in\; [0,1]\; at\; {a \over(a + b)}\]

<p>궁극적으로 알고자 하는 생성자의 Global optimum point는 어디인가</p>

<p>Proposition: $p_g = p_{data}$ (생성자의 분포는 원본 distribution을 따라가게 된다)</p>

<p>Proof:
\(C(G) = \max_{d}V(G, D)\)
\(= E_{x \sim p_{data}}[log(D^*(x)_G] + E_{z \sim p_z(z)}log(1-D^*_G(G(z)))]\)</p>

\[=E_{x\sim p_{data}} [logD^*_G(x)] + E_{x \sim p_g} [log(1-D^*_G(x))]\]

\[D^*_G(x) = {p_{data}(x) \over p_{data}(x) + p_g(x)}\]

<p>\(= E_{x\sim p_{data}(x)} \begin{bmatrix}log{P_{data}(x) \over p_{data}(x) + p_g(x)}\end{bmatrix} + E_{x \sim p_g(x)} \begin{bmatrix} log{p_g(x) \over p_{data}(x) + p_g(x)} \end{bmatrix}\)
\(= E_{x \sim p_{data}(x)} \begin{bmatrix} log{2 \; * \; p_{data}(x) \over P_{data}(x) + p_g(x)}\end{bmatrix} + E_{x \sim p_g(x)} \begin{bmatrix} log {2 \; * \; p_g(x) \over P_{data}(x) + p_g(x)} \end{bmatrix} - log(4)\)			
편의성을 위해 KL Divergence로 변환 (후에 Jensen-Shannon divergence로 변환)</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$KL(p_{data}</td>
          <td> </td>
          <td>p_g) = \int_{-\infty}^\infty p_{data}(x) log({p_{data}(x) \over p_g(x)}) dx$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <p>Kullback-Leibler divergence : 두개의 분포가 있을 떄 두 분포가 얼마나 차이가 나는지에 대한 내용을 수치적으로 표현하기 위해 일반적으로 사용할 수 있는 divergence 공식</p>
  </li>
  <li>Jenson-Shannon divergence : 두개의 분포 p와 q가 있을 때 두 분포의 distance를 구하는데 사용 가능</li>
</ul>

<p>KL divergence는 distance matric으로 활용 불가능 하지만 Jenson-Shannon divergence 는 distance matric 으로 활용 가능하므로 JSD로 변환</p>

<ul>
  <li>
    <p>JSD 공식
\(JSD(p||q) = {1 \over 2}KL(p||{p+q \over 2})+{1\over 2}KL(q||{p+q \over 2})\)</p>
  </li>
  <li>첫번째 텀: $E_{x \sim p_{data}(x)} \begin{bmatrix} log{2 \; * \; p_{data}(x) \over P_{data}(x) + p_g(x)}\end{bmatrix}$</li>
  <li>
    <p>두번째 텀: $E_{x \sim p_g(x)} \begin{bmatrix} log {2 \; * \; p_g(x) \over P_{data}(x) + p_g(x)} \end{bmatrix}$</p>
  </li>
  <li>
    <p>JSD를 사용하기 위해 KL을 JSD형식으로 변환
\(= KL(p_{data} || {p_{data} + p_g (x) \over 2}) + KL(p_g||{p_{data}(x) + p_g(x) \over 2}) - log(4)\)
\(C(G) = 2\; * \; JSD (p_{data} || p_g) - log(4)\)</p>
  </li>
  <li>JSD 는 distance matric이기 때문에 최솟값이 0이며 이는 p와 q가 동일하다는 것을 뜻</li>
  <li>$p_{data}$ 와 $p_q$ 가 동일할 때 JSD는 0이되 사라져 최솟값으로  $-log(4)$ 를 얻을 수 있음</li>
  <li>Global Optimal Point를 얻을 수 있는 유일한 Solution은 생성자의 분포와 $p_{data}$가 동일할 때 즉 생성자가 내뱉는 이미지가 원본 데이터 distribution과 동일할 때 Global Optimal Point를 가질 수 있음</li>
  <li>생성자는 매번 D가 이미 잘 수렴해서 Global Optimal을 가지고 있다고 가정한 상태에서 생성자가 잘 학습된다면 $-log(4)$ 를 가질 수 있도록 잘 수렴해서 $p_{data}$ 와 같은 분포를 가지는 형태로 수렴</li>
</ul>]]></content><author><name>charcoalSkies</name></author><category term="논문 리뷰" /><category term="GAN" /><category term="리뷰" /><category term="논문" /><category term="review" /><summary type="html"><![CDATA[Generative Adversarial Networks]]></summary></entry></feed>